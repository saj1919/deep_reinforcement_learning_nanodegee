{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install unityagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from unityagents import UnityEnvironment\n",
    "from network import Actor, Critic\n",
    "from agent import DDPGAgent, OUNoise, ReplayBuffer\n",
    "from multiagent import MultiAgent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "# env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")\n",
    "env = UnityEnvironment(file_name='./Tennis.app')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "num_agents = len(env_info.agents)\n",
    "shared_replay_buffer = True\n",
    "\n",
    "ma = MultiAgent(action_size, state_size, shared_replay_buffer, num_agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=500, max_t=1000):\n",
    "    all_scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        ma.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations           \n",
    "        scores = np.zeros(num_agents)\n",
    "\n",
    "        for i in range(max_t):\n",
    "            actions = ma.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            rewards = env_info.rewards\n",
    "            next_states = env_info.vector_observations\n",
    "            dones = env_info.local_done\n",
    "\n",
    "            ma.step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            scores += rewards\n",
    "            states = next_states\n",
    "                \n",
    "        avg_score = np.max(scores)\n",
    "        scores_window.append(avg_score)\n",
    "        all_scores.append(avg_score)\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 5 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(ma.ddpg_agents[0].actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(ma.ddpg_agents[0].critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if np.mean(scores_window)>=0.5 and i_episode>=n_episodes:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-5, np.mean(scores_window)))\n",
    "            torch.save(ma.ddpg_agents[0].actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(ma.ddpg_agents[0].critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break \n",
    "            \n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scores = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # import pandas as pd \n",
    "\n",
    "# outdf = pd.DataFrame()\n",
    "# outdf['episode'] = np.arange(len(scores))\n",
    "# outdf['score'] = scores\n",
    "# outdf.plot(x='episode', y='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.ddpg_agents[0].actor_local.load_state_dict(torch.load('checkpoint_actor.pth', map_location='cpu'))\n",
    "ma.ddpg_agents[0].critic_local.load_state_dict(torch.load('checkpoint_critic.pth', map_location='cpu'))\n",
    "\n",
    "ma.ddpg_agents[1].actor_local.load_state_dict(torch.load('checkpoint_actor.pth', map_location='cpu'))\n",
    "ma.ddpg_agents[1].critic_local.load_state_dict(torch.load('checkpoint_critic.pth', map_location='cpu'))\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "for i in range(11):                                         \n",
    "    ma.reset()\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations           \n",
    "    scores = np.zeros(num_agents)\n",
    "    while True:\n",
    "        actions = ma.act(states)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        rewards = env_info.rewards\n",
    "        next_states = env_info.vector_observations\n",
    "        dones = env_info.local_done\n",
    "        scores += rewards\n",
    "        states = next_states\n",
    "        if np.any(dones):                                 \n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
